'use client'

import OpenAI from 'openai'

/**
 * ðŸŒ¿ Bonsai Advanced Computer Vision Engine
 * 
 * State-of-the-art computer vision system for educational content analysis
 * Combines GPT-4V, OCR, mathematical recognition, and real-time processing
 */

export interface VisionAnalysisResult {
  success: boolean
  content: {
    text: string
    mathematicalExpressions: MathExpression[]
    diagrams: DiagramInfo[]
    tables: TableInfo[]
    graphs: GraphInfo[]
  }
  questionAnalysis: {
    type: 'multiple_choice' | 'free_response' | 'grid_in' | 'student_produced'
    subject: 'math' | 'reading' | 'writing' | 'unknown'
    difficulty: 'easy' | 'medium' | 'hard'
    concepts: string[]
    estimatedTime: number
  }
  visualElements: {
    coordinates: BoundingBox[]
    elementTypes: string[]
    layout: LayoutInfo
  }
  confidence: number
  processingTime: number
  error?: string
}

export interface MathExpression {
  latex: string
  text: string
  type: 'equation' | 'formula' | 'expression' | 'function'
  variables: string[]
  constants: number[]
  operations: string[]
  boundingBox: BoundingBox
  confidence: number
}

export interface DiagramInfo {
  type: 'geometric' | 'scientific' | 'chart' | 'flowchart' | 'other'
  description: string
  elements: string[]
  relationships: string[]
  boundingBox: BoundingBox
  confidence: number
}

export interface TableInfo {
  rows: number
  columns: number
  headers: string[]
  data: string[][]
  type: 'data' | 'comparison' | 'statistical'
  boundingBox: BoundingBox
}

export interface GraphInfo {
  type: 'line' | 'bar' | 'scatter' | 'pie' | 'histogram' | 'function'
  title?: string
  xAxis?: AxisInfo
  yAxis?: AxisInfo
  dataPoints: DataPoint[]
  trends: string[]
  boundingBox: BoundingBox
}

export interface AxisInfo {
  label: string
  min: number
  max: number
  unit?: string
  scale: 'linear' | 'logarithmic'
}

export interface DataPoint {
  x: number
  y: number
  label?: string
}

export interface BoundingBox {
  x: number
  y: number
  width: number
  height: number
}

export interface LayoutInfo {
  questionArea: BoundingBox
  choicesArea?: BoundingBox
  passageArea?: BoundingBox
  diagramArea?: BoundingBox
  scratchArea?: BoundingBox
}

export interface ScreenCaptureOptions {
  element?: HTMLElement
  fullPage?: boolean
  quality?: number
  format?: 'png' | 'jpeg' | 'webp'
  scale?: number
  excludeSelectors?: string[]
}

export class BonsaiVisionEngine {
  private openai: OpenAI
  private canvas?: HTMLCanvasElement
  private context?: CanvasRenderingContext2D
  private processingQueue: Map<string, Promise<VisionAnalysisResult>> = new Map()
  
  // Performance optimization
  private cache: Map<string, VisionAnalysisResult> = new Map()
  private lastAnalysisTime = 0
  private analysisThrottle = 2000 // 2 seconds minimum between analyses
  
  constructor(apiKey: string) {
    this.openai = new OpenAI({ 
      apiKey,
      dangerouslyAllowBrowser: true 
    })
    
    this.initializeCanvas()
  }

  private initializeCanvas(): void {
    this.canvas = document.createElement('canvas')
    this.context = this.canvas.getContext('2d')
    
    // Configure for high-quality capture
    this.canvas.style.imageRendering = 'crisp-edges'
    this.canvas.style.imageRendering = '-webkit-optimize-contrast'
  }

  /**
   * Main analysis function - captures and analyzes visual content
   */
  async analyzeVisualContent(
    element?: HTMLElement, 
    options: ScreenCaptureOptions = {}
  ): Promise<VisionAnalysisResult> {
    const startTime = Date.now()
    
    try {
      // Throttle requests for performance
      if (Date.now() - this.lastAnalysisTime < this.analysisThrottle) {
        console.log('ðŸŒ¿ Vision analysis throttled')
        return this.createFallbackResult('Analysis throttled for performance')
      }

      // Capture screenshot
      const imageData = await this.captureScreenshot(element, options)
      
      // Generate cache key
      const cacheKey = await this.generateCacheKey(imageData)
      
      // Check cache
      if (this.cache.has(cacheKey)) {
        console.log('ðŸŒ¿ Returning cached vision analysis')
        return this.cache.get(cacheKey)!
      }

      // Check if analysis already in progress
      if (this.processingQueue.has(cacheKey)) {
        return await this.processingQueue.get(cacheKey)!
      }

      // Start new analysis
      const analysisPromise = this.performVisionAnalysis(imageData)
      this.processingQueue.set(cacheKey, analysisPromise)

      const result = await analysisPromise
      
      // Cache result
      this.cache.set(cacheKey, result)
      this.processingQueue.delete(cacheKey)
      this.lastAnalysisTime = Date.now()

      result.processingTime = Date.now() - startTime
      return result

    } catch (error) {
      console.error('ðŸŒ¿ Vision analysis error:', error)
      return this.createErrorResult(error as Error, Date.now() - startTime)
    }
  }

  /**
   * Advanced screenshot capture with optimization
   */
  private async captureScreenshot(
    element?: HTMLElement, 
    options: ScreenCaptureOptions = {}
  ): Promise<string> {
    const {
      fullPage = false,
      quality = 0.9,
      format = 'png',
      scale = 1,
      excludeSelectors = []
    } = options

    // Determine capture target
    const targetElement = element || document.documentElement

    // Hide excluded elements temporarily
    const hiddenElements: HTMLElement[] = []
    excludeSelectors.forEach(selector => {
      const elements = document.querySelectorAll(selector) as NodeListOf<HTMLElement>
      elements.forEach(el => {
        if (el.style.display !== 'none') {
          hiddenElements.push(el)
          el.style.display = 'none'
        }
      })
    })

    try {
      // Use modern browser APIs for high-quality capture
      if ('html2canvas' in window) {
        return await this.captureWithHtml2Canvas(targetElement, options)
      } else {
        return await this.captureWithNativeAPI(targetElement, options)
      }
    } finally {
      // Restore hidden elements
      hiddenElements.forEach(el => {
        el.style.display = ''
      })
    }
  }

  /**
   * Capture using html2canvas library (if available)
   */
  private async captureWithHtml2Canvas(
    element: HTMLElement, 
    options: ScreenCaptureOptions
  ): Promise<string> {
    // @ts-ignore - html2canvas might be loaded dynamically
    const canvas = await window.html2canvas(element, {
      allowTaint: true,
      useCORS: true,
      scale: options.scale || 1,
      quality: options.quality || 0.9,
      backgroundColor: '#ffffff',
      removeContainer: true,
      logging: false,
      imageTimeout: 15000,
      onclone: (clonedDoc: Document) => {
        // Enhance quality in cloned document
        const style = clonedDoc.createElement('style')
        style.innerHTML = `
          * { 
            -webkit-font-smoothing: antialiased;
            -moz-osx-font-smoothing: grayscale;
            font-rendering: optimizeLegibility;
          }
        `
        clonedDoc.head.appendChild(style)
      }
    })

    return canvas.toDataURL(`image/${options.format || 'png'}`, options.quality || 0.9)
  }

  /**
   * Capture using native browser APIs
   */
  private async captureWithNativeAPI(
    element: HTMLElement, 
    options: ScreenCaptureOptions
  ): Promise<string> {
    if (!this.canvas || !this.context) {
      throw new Error('Canvas not initialized')
    }

    // Get element dimensions
    const rect = element.getBoundingClientRect()
    const scale = options.scale || window.devicePixelRatio || 1

    // Set canvas size
    this.canvas.width = rect.width * scale
    this.canvas.height = rect.height * scale
    this.context.scale(scale, scale)

    // Configure for high quality
    this.context.imageSmoothingEnabled = true
    this.context.imageSmoothingQuality = 'high'
    this.context.textBaseline = 'top'

    // Draw element content (simplified approach)
    this.context.fillStyle = '#ffffff'
    this.context.fillRect(0, 0, rect.width, rect.height)

    // For now, use a more advanced technique with DOM-to-Canvas conversion
    return await this.domToCanvas(element, this.canvas, this.context, options)
  }

  /**
   * Advanced DOM to Canvas conversion
   */
  private async domToCanvas(
    element: HTMLElement,
    canvas: HTMLCanvasElement,
    context: CanvasRenderingContext2D,
    options: ScreenCaptureOptions
  ): Promise<string> {
    // This is a simplified version - in production, you'd use libraries like html2canvas
    // For now, we'll use a different approach: creating a foreign object SVG

    const rect = element.getBoundingClientRect()
    const html = element.outerHTML

    const svgString = `
      <svg xmlns="http://www.w3.org/2000/svg" width="${rect.width}" height="${rect.height}">
        <foreignObject width="100%" height="100%">
          <div xmlns="http://www.w3.org/1999/xhtml" style="font-size: 14px; font-family: -apple-system, BlinkMacSystemFont, sans-serif;">
            ${html}
          </div>
        </foreignObject>
      </svg>
    `

    return new Promise((resolve, reject) => {
      const img = new Image()
      const blob = new Blob([svgString], { type: 'image/svg+xml' })
      const url = URL.createObjectURL(blob)

      img.onload = () => {
        context.drawImage(img, 0, 0)
        URL.revokeObjectURL(url)
        resolve(canvas.toDataURL(`image/${options.format || 'png'}`, options.quality || 0.9))
      }

      img.onerror = () => {
        URL.revokeObjectURL(url)
        // Fallback: convert element to simple text representation
        resolve(this.createTextBasedImage(element, canvas, context, options))
      }

      img.src = url
    })
  }

  /**
   * Fallback: create text-based representation
   */
  private createTextBasedImage(
    element: HTMLElement,
    canvas: HTMLCanvasElement,
    context: CanvasRenderingContext2D,
    options: ScreenCaptureOptions
  ): string {
    // Extract text content and render it
    const text = element.textContent || 'No content available'
    const lines = this.wrapText(text, canvas.width - 40, context)

    context.fillStyle = '#ffffff'
    context.fillRect(0, 0, canvas.width, canvas.height)

    context.fillStyle = '#000000'
    context.font = '14px Arial'
    
    lines.forEach((line, index) => {
      context.fillText(line, 20, 30 + (index * 20))
    })

    return canvas.toDataURL(`image/${options.format || 'png'}`, options.quality || 0.9)
  }

  /**
   * Text wrapping utility
   */
  private wrapText(text: string, maxWidth: number, context: CanvasRenderingContext2D): string[] {
    const words = text.split(' ')
    const lines: string[] = []
    let currentLine = ''

    for (const word of words) {
      const testLine = currentLine + word + ' '
      const metrics = context.measureText(testLine)
      
      if (metrics.width > maxWidth && currentLine !== '') {
        lines.push(currentLine.trim())
        currentLine = word + ' '
      } else {
        currentLine = testLine
      }
    }
    
    if (currentLine.trim() !== '') {
      lines.push(currentLine.trim())
    }

    return lines
  }

  /**
   * Perform comprehensive vision analysis using GPT-4V
   */
  private async performVisionAnalysis(imageData: string): Promise<VisionAnalysisResult> {
    const enhancedPrompt = `
You are an advanced AI vision system for educational content analysis, specifically designed for SAT preparation. Analyze this screenshot of a test question or educational content with extreme precision.

ANALYSIS REQUIREMENTS:
1. **Text Recognition**: Extract ALL text, including handwritten content, mathematical expressions, and any overlay text
2. **Mathematical Content**: Identify equations, formulas, graphs, geometric figures, and mathematical notation
3. **Question Structure**: Determine question type, answer choices, passage content, and navigation elements
4. **Visual Elements**: Analyze diagrams, charts, tables, images, and their educational significance
5. **Educational Assessment**: Evaluate difficulty, concepts tested, and estimated time requirements

RESPONSE FORMAT (JSON):
{
  "content": {
    "text": "Complete extracted text",
    "mathematicalExpressions": [
      {
        "latex": "LaTeX representation",
        "text": "Plain text description", 
        "type": "equation|formula|expression|function",
        "variables": ["x", "y"],
        "constants": [1, 2, 3],
        "operations": ["+", "-", "Ã—"],
        "boundingBox": {"x": 0, "y": 0, "width": 100, "height": 50},
        "confidence": 0.95
      }
    ],
    "diagrams": [
      {
        "type": "geometric|scientific|chart|flowchart|other",
        "description": "Detailed description",
        "elements": ["triangle", "angles", "sides"],
        "relationships": ["perpendicular", "parallel"],
        "boundingBox": {"x": 0, "y": 0, "width": 200, "height": 150},
        "confidence": 0.9
      }
    ],
    "tables": [
      {
        "rows": 3,
        "columns": 4,
        "headers": ["Header1", "Header2"],
        "data": [["cell1", "cell2"], ["cell3", "cell4"]],
        "type": "data|comparison|statistical",
        "boundingBox": {"x": 0, "y": 0, "width": 300, "height": 100}
      }
    ],
    "graphs": [
      {
        "type": "line|bar|scatter|pie|histogram|function",
        "title": "Graph title if visible",
        "xAxis": {"label": "X axis", "min": 0, "max": 10, "scale": "linear"},
        "yAxis": {"label": "Y axis", "min": 0, "max": 100, "scale": "linear"},
        "dataPoints": [{"x": 1, "y": 10, "label": "point1"}],
        "trends": ["increasing", "linear"],
        "boundingBox": {"x": 0, "y": 0, "width": 400, "height": 300}
      }
    ]
  },
  "questionAnalysis": {
    "type": "multiple_choice|free_response|grid_in|student_produced",
    "subject": "math|reading|writing|unknown",
    "difficulty": "easy|medium|hard",
    "concepts": ["algebra", "geometry", "reading_comprehension"],
    "estimatedTime": 90
  },
  "visualElements": {
    "coordinates": [{"x": 0, "y": 0, "width": 100, "height": 50}],
    "elementTypes": ["text", "image", "diagram", "choices"],
    "layout": {
      "questionArea": {"x": 0, "y": 0, "width": 500, "height": 200},
      "choicesArea": {"x": 0, "y": 200, "width": 500, "height": 150},
      "passageArea": {"x": 0, "y": 0, "width": 400, "height": 600},
      "diagramArea": {"x": 400, "y": 100, "width": 300, "height": 200}
    }
  },
  "confidence": 0.95
}

Focus on educational accuracy, mathematical precision, and comprehensive content extraction. Pay special attention to SAT-specific formats and question structures.
`

    try {
      const response = await this.openai.chat.completions.create({
        model: "gpt-4o",
        messages: [
          {
            role: "system",
            content: "You are a specialized AI vision system for educational content analysis with expertise in SAT test formats, mathematical notation, and academic content recognition."
          },
          {
            role: "user",
            content: [
              { type: "text", text: enhancedPrompt },
              {
                type: "image_url",
                image_url: {
                  url: imageData,
                  detail: "high"
                }
              }
            ]
          }
        ],
        max_tokens: 4000,
        temperature: 0.1,
        response_format: { type: "json_object" }
      })

      const analysisResult = response.choices[0]?.message?.content
      if (!analysisResult) {
        throw new Error('No analysis result received from GPT-4V')
      }

      const parsedResult = JSON.parse(analysisResult)
      
      return {
        success: true,
        content: parsedResult.content || {},
        questionAnalysis: parsedResult.questionAnalysis || {},
        visualElements: parsedResult.visualElements || {},
        confidence: parsedResult.confidence || 0.7,
        processingTime: 0 // Will be set by caller
      }

    } catch (error) {
      console.error('ðŸŒ¿ GPT-4V analysis error:', error)
      
      // Fallback to local analysis
      return this.performLocalAnalysis(imageData)
    }
  }

  /**
   * Fallback local analysis using traditional computer vision
   */
  private async performLocalAnalysis(imageData: string): Promise<VisionAnalysisResult> {
    // Load OpenCV.js if available
    if (typeof window !== 'undefined' && 'cv' in window) {
      return this.performOpenCVAnalysis(imageData)
    }

    // Basic analysis without external libraries
    return this.performBasicAnalysis(imageData)
  }

  /**
   * OpenCV.js based analysis
   */
  private async performOpenCVAnalysis(imageData: string): Promise<VisionAnalysisResult> {
    try {
      // @ts-ignore - OpenCV.js global
      const cv = window.cv

      // Create image from data URL
      const img = new Image()
      img.src = imageData

      return new Promise((resolve) => {
        img.onload = () => {
          const canvas = document.createElement('canvas')
          const ctx = canvas.getContext('2d')!
          
          canvas.width = img.width
          canvas.height = img.height
          ctx.drawImage(img, 0, 0)

          // Create OpenCV matrices
          const src = cv.imread(canvas)
          const gray = new cv.Mat()
          const edges = new cv.Mat()

          // Convert to grayscale
          cv.cvtColor(src, gray, cv.COLOR_RGBA2GRAY)

          // Edge detection
          cv.Canny(gray, edges, 50, 150)

          // Find contours (for shapes and diagrams)
          const contours = new cv.MatVector()
          const hierarchy = new cv.Mat()
          cv.findContours(edges, contours, hierarchy, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)

          // Analyze contours for shapes and layout
          const shapes: BoundingBox[] = []
          for (let i = 0; i < contours.size(); i++) {
            const cnt = contours.get(i)
            const rect = cv.boundingRect(cnt)
            
            if (rect.width > 20 && rect.height > 20) {
              shapes.push({
                x: rect.x,
                y: rect.y,
                width: rect.width,
                height: rect.height
              })
            }
          }

          // Clean up
          src.delete()
          gray.delete()
          edges.delete()
          contours.delete()
          hierarchy.delete()

          resolve({
            success: true,
            content: {
              text: 'OpenCV analysis - text extraction limited without OCR',
              mathematicalExpressions: [],
              diagrams: shapes.length > 0 ? [{
                type: 'other',
                description: `Detected ${shapes.length} potential diagram elements`,
                elements: [`${shapes.length} shapes detected`],
                relationships: [],
                boundingBox: shapes[0] || { x: 0, y: 0, width: 0, height: 0 },
                confidence: 0.6
              }] : [],
              tables: [],
              graphs: []
            },
            questionAnalysis: {
              type: 'multiple_choice',
              subject: 'unknown',
              difficulty: 'medium',
              concepts: ['visual_analysis'],
              estimatedTime: 60
            },
            visualElements: {
              coordinates: shapes,
              elementTypes: ['detected_shapes'],
              layout: {
                questionArea: { x: 0, y: 0, width: img.width, height: img.height }
              }
            },
            confidence: 0.6,
            processingTime: 0
          })
        }
      })

    } catch (error) {
      console.error('ðŸŒ¿ OpenCV analysis error:', error)
      return this.performBasicAnalysis(imageData)
    }
  }

  /**
   * Basic analysis without external libraries
   */
  private async performBasicAnalysis(imageData: string): Promise<VisionAnalysisResult> {
    return {
      success: true,
      content: {
        text: 'Basic analysis - limited capabilities without advanced vision libraries',
        mathematicalExpressions: [],
        diagrams: [],
        tables: [],
        graphs: []
      },
      questionAnalysis: {
        type: 'multiple_choice',
        subject: 'unknown', 
        difficulty: 'medium',
        concepts: ['basic_analysis'],
        estimatedTime: 60
      },
      visualElements: {
        coordinates: [],
        elementTypes: ['unknown'],
        layout: {
          questionArea: { x: 0, y: 0, width: 800, height: 600 }
        }
      },
      confidence: 0.3,
      processingTime: 0
    }
  }

  /**
   * Generate cache key for analysis result
   */
  private async generateCacheKey(imageData: string): Promise<string> {
    // Create a hash of the image data for caching
    const encoder = new TextEncoder()
    const data = encoder.encode(imageData.slice(0, 1000)) // Use first 1000 chars for efficiency
    const hashBuffer = await crypto.subtle.digest('SHA-256', data)
    const hashArray = Array.from(new Uint8Array(hashBuffer))
    return hashArray.map(b => b.toString(16).padStart(2, '0')).join('')
  }

  /**
   * Create fallback result for performance reasons
   */
  private createFallbackResult(reason: string): VisionAnalysisResult {
    return {
      success: false,
      content: {
        text: reason,
        mathematicalExpressions: [],
        diagrams: [],
        tables: [],
        graphs: []
      },
      questionAnalysis: {
        type: 'multiple_choice',
        subject: 'unknown',
        difficulty: 'medium',
        concepts: [],
        estimatedTime: 60
      },
      visualElements: {
        coordinates: [],
        elementTypes: [],
        layout: {
          questionArea: { x: 0, y: 0, width: 0, height: 0 }
        }
      },
      confidence: 0,
      processingTime: 0,
      error: reason
    }
  }

  /**
   * Create error result
   */
  private createErrorResult(error: Error, processingTime: number): VisionAnalysisResult {
    return {
      success: false,
      content: {
        text: 'Vision analysis failed',
        mathematicalExpressions: [],
        diagrams: [],
        tables: [],
        graphs: []
      },
      questionAnalysis: {
        type: 'multiple_choice',
        subject: 'unknown',
        difficulty: 'medium',
        concepts: [],
        estimatedTime: 60
      },
      visualElements: {
        coordinates: [],
        elementTypes: [],
        layout: {
          questionArea: { x: 0, y: 0, width: 0, height: 0 }
        }
      },
      confidence: 0,
      processingTime,
      error: error.message
    }
  }

  /**
   * Clear cache to free memory
   */
  public clearCache(): void {
    this.cache.clear()
    this.processingQueue.clear()
    console.log('ðŸŒ¿ Vision engine cache cleared')
  }

  /**
   * Get cache statistics
   */
  public getCacheStats(): { size: number; keys: string[] } {
    return {
      size: this.cache.size,
      keys: Array.from(this.cache.keys())
    }
  }
}